{"cells":[{"cell_type":"markdown","id":"383de715","metadata":{},"source":["Coursework - Task 3"]},{"cell_type":"code","execution_count":1,"id":"82bc79fd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(60000, 785)\n","   label     0\n","0      0  6000\n","1      1  6000\n","2      2  6000\n","3      3  6000\n","4      4  6000\n","5      5  6000\n","6      6  6000\n","7      7  6000\n","8      8  6000\n","9      9  6000\n","(784, 60000)\n","(10, 60000)\n"]}],"source":["# Importing the Librarys\n","import pandas as pd\n","import numpy as np\n","\n","\n","# Importing and reading the data from the csv file\n","np.random.seed(42)\n","data = pd.read_csv('fashion-mnist_train.csv')\n","print(data.shape)\n","data = data.sample(frac=1)\n","print(data[['label']].groupby('label').size().reset_index())\n","\n","# Converting the clothes categories to one-hot encoded vectors and prepares the training and testing\n","one_hot = pd.get_dummies(data['label'].unique())\n","one_hot['label'] = one_hot.index\n","\n","data = pd.merge(data,one_hot)\n","data = data.sample(frac=1)\n","data_train = data\n","data_test = pd.read_csv('fashion-mnist_test.csv')\n","data_test = pd.merge(data_test,one_hot)\n","data_train.drop('label',axis=1,inplace=True)\n","data_test.drop('label',axis=1,inplace=True)\n","\n","# Creating the training and testing set\n","X_train = np.array(data_train.drop([0,1,2,3,4,5,6,7,8,9],axis=1).values)/255\n","y_train = np.array(data_train[[0,1,2,3,4,5,6,7,8,9]].values)\n","X_test = np.array(data_test.drop([0,1,2,3,4,5,6,7,8,9],axis=1).values)/255\n","y_test = np.array(data_test[[0,1,2,3,4,5,6,7,8,9]].values)\n","\n","X_train = X_train.T\n","y_train = y_train.T\n","print(X_train.shape)\n","print(y_train.shape)\n","X_test = X_test.T\n","y_test = y_test.T"]},{"cell_type":"markdown","id":"aab8c25a","metadata":{},"source":["Implementing Sigmoid, Relu and Softmax layers"]},{"cell_type":"code","execution_count":2,"id":"abad64d3","metadata":{},"outputs":[],"source":["# Initializing the Weight Matrices\n","def sigmoid(x):\n","    y = 1./(1+np.exp(-x))\n","    return y\n","\n","def sigmoid_backpass(x):\n","    s = 1/(1+np.exp(-x))\n","    dx = s * (1-s)\n","    return dx\n","\n","def relu(x):\n","    y = np.maximum(0,x)\n","    return y\n","\n","def relu_backpass(x):\n","    x[x<=0] = 0\n","    x[x>0] = 1\n","    return x\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    total = e_x.sum(axis=0)\n","    return (e_x / total) "]},{"cell_type":"markdown","id":"2f0d5c59","metadata":{},"source":["â€¢\tImplement an optimizer (e.g. Adam) "]},{"cell_type":"code","execution_count":3,"id":"59b7aae8","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/var/folders/1x/151x284x3hv6jvqsv5z9bttc0000gn/T/ipykernel_39039/2115661457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}],"source":["# Optimizer\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),lr= 0.1)"]},{"cell_type":"markdown","id":"666fe0e8","metadata":{},"source":["Implementing a fully parameterizable network"]},{"cell_type":"code","execution_count":4,"id":"aea64c6f","metadata":{},"outputs":[],"source":["# Initialize a network\n","def initialize_network(inputs, hidden, outputs):\n","    network = list()\n","    #Define hidden layers of the network. Weights are randomised\n","    hidden_layer = [{'weights':[random() for i in range(inputs + 1)]} for i in range(hidden)]\n","    #Add the defined hidden layer to the network. More layers can easily be added\n","    network.append(hidden_layer)\n","    #Define the output layer of the network. Weights are randomised\n","    output_layer = [{'weights':[random() for i in range(hidden + 1)]} for i in range(outputs)]\n","    #Add the defined hidden layer to the network.\n","    network.append(output_layer)\n","    return network\n"," \n","# Calculate neuron activation for an input\n","def activate(weights, inputs):\n","    activation = 0\n","    # Neuron activation is calculated as the weighted sum of inputs\n","    for i in range(len(weights)-1):\n","        activation += weights[i] * inputs[i]\n","    #Still have to include the weight that has now input to multiply with\n","    activation += weights[-1]\n","    return activation\n","\n","# Return the derivative of an output\n","def transfer_derivative(x):\n","    y = x * (1.0 - x)\n","    return y\n","\n","# Forward propagate inputs to a network output\n","def forward_propagate(network, row):\n","    inputs = row\n","    for layer in network:\n","        new_inputs = []\n","        #For every neuron in a layer, calculate the activation and active it with an activation function to get new inputs\n","        for neuron in layer:\n","            activation = activate(neuron['weights'], inputs)\n","            neuron['output'] = sigmoid(activation)\n","            #neuron['output'] = relu(activation)\n","            #neuron['output'] = softmax(activation)\n","            new_inputs.append(neuron['output'])\n","        # replace old input with new input for the next layer in the network\n","        inputs = new_inputs\n","    return inputs"]},{"cell_type":"markdown","id":"1d067310","metadata":{},"source":["Training using Backpropagation"]},{"cell_type":"code","execution_count":5,"id":"3b571d19","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test:  0\n","Testing Accuracy: 0.2\n","Training Accuracy: 0.22213333333333332\n","\n","\n","Test:  1\n","Testing Accuracy: 0.3264\n","Training Accuracy: 0.1991\n","\n","\n","Test:  2\n","Testing Accuracy: 0.2023\n","Training Accuracy: 0.13721666666666665\n","\n","\n","Test:  3\n","Testing Accuracy: 0.2683\n","Training Accuracy: 0.18866666666666668\n","\n","\n","Test:  4\n","Testing Accuracy: 0.2405\n","Training Accuracy: 0.5350833333333334\n","\n","\n","Test:  5\n","Testing Accuracy: 0.2375\n","Training Accuracy: 0.5619833333333333\n","\n","\n","Test:  6\n","Testing Accuracy: 0.2964\n","Training Accuracy: 0.6157166666666667\n","\n","\n","Test:  7\n","Testing Accuracy: 0.305\n","Training Accuracy: 0.6490333333333334\n","\n","\n","Test:  8\n","Testing Accuracy: 0.315\n","Training Accuracy: 0.6442666666666667\n","\n","\n","Test:  9\n","Testing Accuracy: 0.3095\n","Training Accuracy: 0.6371666666666667\n","\n","\n","Test:  10\n","Testing Accuracy: 0.3176\n","Training Accuracy: 0.6331\n","\n","\n","Test:  11\n","Testing Accuracy: 0.3779\n","Training Accuracy: 0.6738\n","\n","\n","Test:  12\n","Testing Accuracy: 0.1095\n","Training Accuracy: 0.6487166666666667\n","\n","\n","Test:  13\n","Testing Accuracy: 0.2999\n","Training Accuracy: 0.6784166666666667\n","\n","\n","Test:  14\n","Testing Accuracy: 0.3072\n","Training Accuracy: 0.6613\n","\n","\n","Test:  15\n","Testing Accuracy: 0.414\n","Training Accuracy: 0.6049666666666667\n","\n","\n","Test:  16\n","Testing Accuracy: 0.392\n","Training Accuracy: 0.6754\n","\n","\n","Test:  17\n","Testing Accuracy: 0.2031\n","Training Accuracy: 0.6233\n","\n","\n","Test:  18\n","Testing Accuracy: 0.1901\n","Training Accuracy: 0.9548166666666666\n","\n","\n","Test:  19\n","Testing Accuracy: 0.098\n","Training Accuracy: 0.9465666666666667\n","\n","\n","Test:  20\n","Testing Accuracy: 0.2021\n","Training Accuracy: 0.6145333333333334\n","\n","\n","Test:  21\n","Testing Accuracy: 0.2045\n","Training Accuracy: 0.6212833333333333\n","\n","\n","Test:  22\n","Testing Accuracy: 0.2033\n","Training Accuracy: 0.9646833333333333\n","\n","\n","Test:  23\n","Testing Accuracy: 0.3927\n","Training Accuracy: 0.6074333333333334\n","\n","\n","Test:  24\n","Testing Accuracy: 0.1994\n","Training Accuracy: 0.9797833333333333\n","\n","\n","Test:  25\n","Testing Accuracy: 0.1145\n","Training Accuracy: 0.9127833333333333\n","\n","\n","Test:  26\n","Testing Accuracy: 0.4108\n","Training Accuracy: 0.61115\n","\n","\n","Test:  27\n","Testing Accuracy: 0.3922\n","Training Accuracy: 0.6031666666666666\n","\n","\n","Test:  28\n","Testing Accuracy: 0.3363\n","Training Accuracy: 0.9443\n","\n","\n","Test:  29\n","Testing Accuracy: 0.4069\n","Training Accuracy: 0.5779666666666666\n","\n","\n","Test:  30\n","Testing Accuracy: 0.2025\n","Training Accuracy: 0.60235\n","\n","\n","Test:  31\n","Testing Accuracy: 0.3878\n","Training Accuracy: 0.94695\n","\n","\n","Test:  32\n","Testing Accuracy: 0.1972\n","Training Accuracy: 0.6053666666666667\n","\n","\n","Test:  33\n","Testing Accuracy: 0.2059\n","Training Accuracy: 0.61175\n","\n","\n","Test:  34\n","Testing Accuracy: 0.3868\n","Training Accuracy: 0.9658166666666667\n","\n","\n","Test:  35\n","Testing Accuracy: 0.2018\n","Training Accuracy: 0.9757833333333333\n","\n","\n","Test:  36\n","Testing Accuracy: 0.2156\n","Training Accuracy: 0.6038833333333333\n","\n","\n","Test:  37\n","Testing Accuracy: 0.2017\n","Training Accuracy: 0.6078166666666667\n","\n","\n","Test:  38\n","Testing Accuracy: 0.1964\n","Training Accuracy: 0.60435\n","\n","\n","Test:  39\n","Testing Accuracy: 0.2055\n","Training Accuracy: 0.6066666666666667\n","\n","\n","Test:  40\n","Testing Accuracy: 0.2062\n","Training Accuracy: 0.5971\n","\n","\n","Test:  41\n","Testing Accuracy: 0.3939\n","Training Accuracy: 0.5958333333333333\n","\n","\n","Test:  42\n","Testing Accuracy: 0.2039\n","Training Accuracy: 0.6134666666666667\n","\n","\n","Test:  43\n","Testing Accuracy: 0.1979\n","Training Accuracy: 0.97665\n","\n","\n","Test:  44\n","Testing Accuracy: 0.2102\n","Training Accuracy: 0.5976\n","\n","\n","Test:  45\n","Testing Accuracy: 0.1998\n","Training Accuracy: 0.9622166666666667\n","\n","\n","Test:  46\n","Testing Accuracy: 0.2032\n","Training Accuracy: 0.6197333333333334\n","\n","\n","Test:  47\n","Testing Accuracy: 0.2063\n","Training Accuracy: 0.6055833333333334\n","\n","\n","Test:  48\n","Testing Accuracy: 0.2087\n","Training Accuracy: 0.6099666666666667\n","\n","\n","Test:  49\n","Testing Accuracy: 0.1111\n","Training Accuracy: 0.5839333333333333\n","\n","\n","Test:  50\n","Testing Accuracy: 0.2063\n","Training Accuracy: 0.6173166666666666\n","\n","\n","Test:  51\n","Testing Accuracy: 0.2069\n","Training Accuracy: 0.6008833333333333\n","\n","\n","Test:  52\n","Testing Accuracy: 0.1205\n","Training Accuracy: 0.5526833333333333\n","\n","\n","Test:  53\n","Testing Accuracy: 0.1974\n","Training Accuracy: 0.6035333333333334\n","\n","\n","Test:  54\n","Testing Accuracy: 0.2076\n","Training Accuracy: 0.60035\n","\n","\n","Test:  55\n","Testing Accuracy: 0.1883\n","Training Accuracy: 0.9679166666666666\n","\n","\n","Test:  56\n","Testing Accuracy: 0.1127\n","Training Accuracy: 0.6021666666666666\n","\n","\n","Test:  57\n","Testing Accuracy: 0.1006\n","Training Accuracy: 0.57785\n","\n","\n","Test:  58\n","Testing Accuracy: 0.1132\n","Training Accuracy: 0.5764333333333334\n","\n","\n","Test:  59\n","Testing Accuracy: 0.1912\n","Training Accuracy: 0.59375\n","\n","\n","Test:  60\n","Testing Accuracy: 0.2115\n","Training Accuracy: 0.6118166666666667\n","\n","\n","Test:  61\n","Testing Accuracy: 0.1249\n","Training Accuracy: 0.5384333333333333\n","\n","\n","Test:  62\n","Testing Accuracy: 0.1038\n","Training Accuracy: 0.6070166666666666\n","\n","\n","Test:  63\n","Testing Accuracy: 0.1938\n","Training Accuracy: 0.5934666666666667\n","\n","\n","Test:  64\n","Testing Accuracy: 0.1129\n","Training Accuracy: 0.6027666666666667\n","\n","\n","Test:  65\n","Testing Accuracy: 0.1115\n","Training Accuracy: 0.5811333333333333\n","\n","\n","Test:  66\n","Testing Accuracy: 0.1182\n","Training Accuracy: 0.5846666666666667\n","\n","\n","Test:  67\n","Testing Accuracy: 0.1024\n","Training Accuracy: 0.6002666666666666\n","\n","\n","Test:  68\n","Testing Accuracy: 0.1063\n","Training Accuracy: 0.5852\n","\n","\n","Test:  69\n","Testing Accuracy: 0.1163\n","Training Accuracy: 0.5964833333333334\n","\n","\n","Test:  70\n","Testing Accuracy: 0.1137\n","Training Accuracy: 0.5732333333333334\n","\n","\n","Test:  71\n","Testing Accuracy: 0.115\n","Training Accuracy: 0.5843833333333334\n","\n","\n","Test:  72\n","Testing Accuracy: 0.2941\n","Training Accuracy: 0.5999166666666667\n","\n","\n","Test:  73\n","Testing Accuracy: 0.3709\n","Training Accuracy: 0.5911666666666666\n","\n","\n","Test:  74\n","Testing Accuracy: 0.2015\n","Training Accuracy: 0.6042666666666666\n","\n","\n","Test:  75\n","Testing Accuracy: 0.1984\n","Training Accuracy: 0.5823666666666667\n","\n","\n","Test:  76\n","Testing Accuracy: 0.1909\n","Training Accuracy: 0.5848333333333333\n","\n","\n","Test:  77\n","Testing Accuracy: 0.1009\n","Training Accuracy: 0.5957666666666667\n","\n","\n","Test:  78\n","Testing Accuracy: 0.102\n","Training Accuracy: 0.60865\n","\n","\n","Test:  79\n","Testing Accuracy: 0.2051\n","Training Accuracy: 0.5935833333333334\n","\n","\n","Test:  80\n","Testing Accuracy: 0.2069\n","Training Accuracy: 0.6119\n","\n","\n","Test:  81\n","Testing Accuracy: 0.1007\n","Training Accuracy: 0.59555\n","\n","\n","Test:  82\n","Testing Accuracy: 0.182\n","Training Accuracy: 0.5506666666666666\n","\n","\n","Test:  83\n","Testing Accuracy: 0.2117\n","Training Accuracy: 0.6009833333333333\n","\n","\n","Test:  84\n","Testing Accuracy: 0.202\n","Training Accuracy: 0.6084666666666667\n","\n","\n","Test:  85\n","Testing Accuracy: 0.2082\n","Training Accuracy: 0.58745\n","\n","\n","Test:  86\n","Testing Accuracy: 0.0979\n","Training Accuracy: 0.6031666666666666\n","\n","\n","Test:  87\n","Testing Accuracy: 0.3074\n","Training Accuracy: 0.5835833333333333\n","\n","\n","Test:  88\n","Testing Accuracy: 0.2957\n","Training Accuracy: 0.6036333333333334\n","\n","\n","Test:  89\n","Testing Accuracy: 0.1113\n","Training Accuracy: 0.5932833333333334\n","\n","\n","Test:  90\n","Testing Accuracy: 0.2359\n","Training Accuracy: 0.75105\n","\n","\n","Test:  91\n","Testing Accuracy: 0.2071\n","Training Accuracy: 0.59915\n","\n","\n","Test:  92\n","Testing Accuracy: 0.2998\n","Training Accuracy: 0.61995\n","\n","\n","Test:  93\n","Testing Accuracy: 0.306\n","Training Accuracy: 0.5670166666666666\n","\n","\n","Test:  94\n","Testing Accuracy: 0.1819\n","Training Accuracy: 0.5569833333333334\n","\n","\n","Test:  95\n","Testing Accuracy: 0.2022\n","Training Accuracy: 0.6082833333333333\n","\n","\n","Test:  96\n","Testing Accuracy: 0.2976\n","Training Accuracy: 0.6031\n","\n","\n","Test:  97\n","Testing Accuracy: 0.3036\n","Training Accuracy: 0.6113333333333333\n","\n","\n","Test:  98\n","Testing Accuracy: 0.3051\n","Training Accuracy: 0.6109333333333333\n","\n","\n","Test:  99\n","Testing Accuracy: 0.3073\n","Training Accuracy: 0.5988\n","\n","\n","Test:  100\n","Testing Accuracy: 0.2064\n","Training Accuracy: 0.5955833333333334\n","\n","\n","Test:  101\n","Testing Accuracy: 0.4906\n","Training Accuracy: 0.5944\n","\n","\n","Test:  102\n","Testing Accuracy: 0.4996\n","Training Accuracy: 0.6097333333333333\n","\n","\n","Test:  103\n","Testing Accuracy: 0.2897\n","Training Accuracy: 0.5925333333333334\n","\n","\n","Test:  104\n","Testing Accuracy: 0.5034\n","Training Accuracy: 0.6110833333333333\n","\n","\n","Test:  105\n","Testing Accuracy: 0.2963\n","Training Accuracy: 0.5954666666666667\n","\n","\n","Test:  106\n","Testing Accuracy: 0.2035\n","Training Accuracy: 0.6008333333333333\n","\n","\n","Test:  107\n","Testing Accuracy: 0.4004\n","Training Accuracy: 0.6140333333333333\n","\n","\n","Test:  108\n","Testing Accuracy: 0.2778\n","Training Accuracy: 0.5749833333333333\n","\n","\n","Test:  109\n","Testing Accuracy: 0.4926\n","Training Accuracy: 0.59945\n","\n","\n","Test:  110\n","Testing Accuracy: 0.3979\n","Training Accuracy: 0.6011333333333333\n","\n","\n","Test:  111\n","Testing Accuracy: 0.4931\n","Training Accuracy: 0.6019833333333333\n","\n","\n","Test:  112\n","Testing Accuracy: 0.3076\n","Training Accuracy: 0.6075333333333334\n","\n","\n","Test:  113\n","Testing Accuracy: 0.4078\n","Training Accuracy: 0.6076166666666667\n","\n","\n","Test:  114\n","Testing Accuracy: 0.485\n","Training Accuracy: 0.5727166666666667\n","\n","\n","Test:  115\n","Testing Accuracy: 0.4039\n","Training Accuracy: 0.6129166666666667\n","\n","\n","Test:  116\n","Testing Accuracy: 0.3976\n","Training Accuracy: 0.61145\n","\n","\n","Test:  117\n","Testing Accuracy: 0.4038\n","Training Accuracy: 0.57355\n","\n","\n","Test:  118\n","Testing Accuracy: 0.485\n","Training Accuracy: 0.5449666666666667\n","\n","\n","Test:  119\n","Testing Accuracy: 0.4007\n","Training Accuracy: 0.5993166666666667\n","\n","\n","Test:  120\n","Testing Accuracy: 0.405\n","Training Accuracy: 0.6140833333333333\n","\n","\n","Test:  121\n","Testing Accuracy: 0.4841\n","Training Accuracy: 0.5754166666666667\n","\n","\n","Test:  122\n","Testing Accuracy: 0.4007\n","Training Accuracy: 0.59695\n","\n","\n","Test:  123\n","Testing Accuracy: 0.4014\n","Training Accuracy: 0.6202\n","\n","\n","Test:  124\n","Testing Accuracy: 0.4\n","Training Accuracy: 0.5935666666666667\n","\n","\n","Test:  125\n","Testing Accuracy: 0.3963\n","Training Accuracy: 0.6044166666666667\n","\n","\n","Test:  126\n","Testing Accuracy: 0.3971\n","Training Accuracy: 0.6117833333333333\n","\n","\n","Test:  127\n","Testing Accuracy: 0.3932\n","Training Accuracy: 0.58425\n","\n","\n","Test:  128\n","Testing Accuracy: 0.4041\n","Training Accuracy: 0.5785166666666667\n","\n","\n","Test:  129\n","Testing Accuracy: 0.397\n","Training Accuracy: 0.6000333333333333\n","\n","\n","Test:  130\n","Testing Accuracy: 0.2978\n","Training Accuracy: 0.59555\n","\n","\n","Test:  131\n","Testing Accuracy: 0.3975\n","Training Accuracy: 0.6198166666666667\n","\n","\n","Test:  132\n","Testing Accuracy: 0.3968\n","Training Accuracy: 0.6041666666666666\n","\n","\n","Test:  133\n","Testing Accuracy: 0.4008\n","Training Accuracy: 0.6122\n","\n","\n","Test:  134\n","Testing Accuracy: 0.4034\n","Training Accuracy: 0.6006666666666667\n","\n","\n","Test:  135\n","Testing Accuracy: 0.3929\n","Training Accuracy: 0.5770166666666666\n","\n","\n","Test:  136\n","Testing Accuracy: 0.396\n","Training Accuracy: 0.6089\n","\n","\n","Test:  137\n","Testing Accuracy: 0.4005\n","Training Accuracy: 0.5751166666666667\n","\n","\n","Test:  138\n","Testing Accuracy: 0.3903\n","Training Accuracy: 0.5981\n","\n","\n","Test:  139\n","Testing Accuracy: 0.3997\n","Training Accuracy: 0.58575\n","\n","\n","Test:  140\n","Testing Accuracy: 0.5122\n","Training Accuracy: 0.6194333333333333\n","\n","\n","Test:  141\n","Testing Accuracy: 0.3959\n","Training Accuracy: 0.5849166666666666\n","\n","\n","Test:  142\n","Testing Accuracy: 0.3988\n","Training Accuracy: 0.593\n","\n","\n","Test:  143\n","Testing Accuracy: 0.4101\n","Training Accuracy: 0.6138166666666667\n","\n","\n","Test:  144\n","Testing Accuracy: 0.2983\n","Training Accuracy: 0.5969833333333333\n","\n","\n","Test:  145\n","Testing Accuracy: 0.4035\n","Training Accuracy: 0.5966666666666667\n","\n","\n","Test:  146\n","Testing Accuracy: 0.3999\n","Training Accuracy: 0.6072\n","\n","\n","Test:  147\n","Testing Accuracy: 0.4005\n","Training Accuracy: 0.60305\n","\n","\n","Test:  148\n","Testing Accuracy: 0.3985\n","Training Accuracy: 0.6103666666666666\n","\n","\n","Test:  149\n","Testing Accuracy: 0.405\n","Training Accuracy: 0.5965833333333334\n","\n","\n","Test:  150\n","Testing Accuracy: 0.3978\n","Training Accuracy: 0.6055666666666667\n","\n","\n","Test:  151\n","Testing Accuracy: 0.4005\n","Training Accuracy: 0.615\n","\n","\n","Test:  152\n","Testing Accuracy: 0.3952\n","Training Accuracy: 0.6098166666666667\n","\n","\n","Test:  153\n","Testing Accuracy: 0.394\n","Training Accuracy: 0.58305\n","\n","\n","Test:  154\n","Testing Accuracy: 0.3927\n","Training Accuracy: 0.58575\n","\n","\n","Test:  155\n","Testing Accuracy: 0.4085\n","Training Accuracy: 0.5788\n","\n","\n","Test:  156\n","Testing Accuracy: 0.402\n","Training Accuracy: 0.5747666666666666\n","\n","\n","Test:  157\n","Testing Accuracy: 0.4036\n","Training Accuracy: 0.58915\n","\n","\n","Test:  158\n","Testing Accuracy: 0.4021\n","Training Accuracy: 0.6004833333333334\n","\n","\n","Test:  159\n","Testing Accuracy: 0.396\n","Training Accuracy: 0.5964166666666667\n","\n","\n","Test:  160\n","Testing Accuracy: 0.397\n","Training Accuracy: 0.5985\n","\n","\n","Test:  161\n","Testing Accuracy: 0.3864\n","Training Accuracy: 0.5581333333333334\n","\n","\n","Test:  162\n","Testing Accuracy: 0.3327\n","Training Accuracy: 0.5967666666666667\n","\n","\n","Test:  163\n","Testing Accuracy: 0.4019\n","Training Accuracy: 0.5976\n","\n","\n","Test:  164\n","Testing Accuracy: 0.3955\n","Training Accuracy: 0.6111166666666666\n","\n","\n","Test:  165\n","Testing Accuracy: 0.403\n","Training Accuracy: 0.5803166666666667\n","\n","\n","Test:  166\n","Testing Accuracy: 0.4068\n","Training Accuracy: 0.60145\n","\n","\n","Test:  167\n","Testing Accuracy: 0.3798\n","Training Accuracy: 0.5768833333333333\n","\n","\n","Test:  168\n","Testing Accuracy: 0.4017\n","Training Accuracy: 0.61065\n","\n","\n","Test:  169\n","Testing Accuracy: 0.399\n","Training Accuracy: 0.58085\n","\n","\n","Test:  170\n","Testing Accuracy: 0.2988\n","Training Accuracy: 0.6119666666666667\n","\n","\n","Test:  171\n","Testing Accuracy: 0.4013\n","Training Accuracy: 0.6017166666666667\n","\n","\n","Test:  172\n","Testing Accuracy: 0.4018\n","Training Accuracy: 0.5963\n","\n","\n","Test:  173\n","Testing Accuracy: 0.3983\n","Training Accuracy: 0.60655\n","\n","\n","Test:  174\n","Testing Accuracy: 0.3959\n","Training Accuracy: 0.60855\n","\n","\n","Test:  175\n","Testing Accuracy: 0.4014\n","Training Accuracy: 0.60835\n","\n","\n","Test:  176\n","Testing Accuracy: 0.4016\n","Training Accuracy: 0.6175666666666667\n","\n","\n","Test:  177\n","Testing Accuracy: 0.3979\n","Training Accuracy: 0.5711166666666667\n","\n","\n","Test:  178\n","Testing Accuracy: 0.302\n","Training Accuracy: 0.5897166666666667\n","\n","\n","Test:  179\n","Testing Accuracy: 0.3921\n","Training Accuracy: 0.5889\n","\n","\n","Test:  180\n","Testing Accuracy: 0.3975\n","Training Accuracy: 0.5918333333333333\n","\n","\n","Test:  181\n","Testing Accuracy: 0.3914\n","Training Accuracy: 0.5985666666666667\n","\n","\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/1x/151x284x3hv6jvqsv5z9bttc0000gn/T/ipykernel_39039/2114954289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mm_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import random\n","random.seed(42)\n","w1 = np.random.rand(128,784)/np.sqrt(784)\n","b0 = np.zeros((128,1))/np.sqrt(784)\n","w2 = np.random.rand(10,128)/np.sqrt(128)\n","b1 = np.zeros((10,1))/np.sqrt(128)\n","loss=[]\n","batches = 1000\n","\n","lr = 0.1\n","batch_size = 200\n","beta = 0.9\n","count = 0\n","epochs = 500\n","\n","loss_weight_dict = {\n","\n","}\n","\n","# For every one cycle of training the neural network\n","for i in range(epochs):\n","    permutation = np.random.permutation(X_train.shape[1])\n","    X_train_shuffled = X_train[:, permutation]\n","    Y_train_shuffled = y_train[:, permutation]\n","    \n","    for j in range(batches):\n","        \n","        begin = j * batch_size\n","        end = min(begin + batch_size, X_train.shape[1] - 1)\n","        if begin>end:\n","            continue\n","        #optimizer.zero_grad()\n","        X = X_train_shuffled[:, begin:end]\n","        Y = Y_train_shuffled[:, begin:end]\n","        m_batch = end - begin\n","        x1 = sigmoid(w1@X+b0)\n","        x2 = softmax(w2@x1+b1)\n","\n","        delta_2 = (x2-Y)\n","        delta_1 = np.multiply(w2.T@delta_2, np.multiply(x1,1-x1))\n","        if i==0 :\n","            dW1 = delta_1@X.T\n","            dW2 = delta_2@x1.T\n","            db0 = np.sum(delta_1,axis=1,keepdims=True)\n","            db1 = np.sum(delta_2,axis=1,keepdims=True)\n","        else:\n","            dW1_old = dW1\n","            dW2_old = dW2\n","            db0_old = db0\n","            db1_old = db1\n","            dW1 = delta_1@X.T\n","            dW2 = delta_2@x1.T\n","            db0 = np.sum(delta_1,axis=1,keepdims=True)\n","            db1 = np.sum(delta_2,axis=1,keepdims=True)\n","            \n","            # Using the past gradients to calculate the present gradients\n","            dW1 = (beta * dW1_old + (1. - beta) * dW1)\n","            db0 = (beta * db0_old + (1. - beta) * db0)\n","            dW2 = (beta * dW2_old + (1. - beta) * dW2)\n","            db1 = (beta * db1_old + (1. - beta) * db1)\n","\n","\n","        w1 = w1 - (1./m_batch)*(dW1)*lr\n","        b0 = b0 - (1./m_batch)*(db0)*(lr)\n","        w2 = w2 - (1./m_batch)*(dW2)*lr\n","        b1 = b1 - (1./m_batch)*(db1)*(lr)\n","    \n","    #optimizer.step()\n","\n","    x1 = sigmoid(w1@X_train+b0)\n","    x2_train = softmax(w2@x1+b1)\n","    x2_train_df = pd.DataFrame(x2_train)\n","    x2_train_df = (x2_train_df == x2_train_df.max()).astype(int)\n","    x2_train_df = x2_train_df.transpose()\n","    x2_train_df = pd.merge(x2_train_df,one_hot)\n","    x2_train_df = x2_train_df[['label']]\n","    y_train_df = pd.merge(pd.DataFrame(y_train.T),one_hot)\n","    x2_train_df['label_actual'] = y_train_df['label']\n","    train_accuracy = np.sum(x2_train_df['label_actual']==x2_train_df['label'])/x2_train_df.shape[0]\n","    add_loss = {\n","        'loss' : -np.mean(np.multiply(y_train,np.log(x2_train))),\n","        'weight_1' : w1,\n","        'weight_2':w2,\n","        'b0' : b0,\n","        'b1': b1,\n","        'train_accuracy': train_accuracy\n","    }\n","     \n","    x1 = sigmoid(w1@X_test+b0)\n","    x2_test = softmax(w2@x1+b1)\n","    x2_test_df = pd.DataFrame(x2_test)\n","    x2_test_df = (x2_test_df == x2_test_df.max()).astype(int)\n","    x2_test_df = x2_test_df.transpose()\n","    x2_test_df = pd.merge(x2_test_df,one_hot)\n","    x2_test_df = x2_test_df[['label']]\n","    y_test_df = pd.merge(pd.DataFrame(y_test.T),one_hot)\n","    x2_test_df['label_actual'] = y_test_df['label']\n","    test_accuracy = np.sum(x2_test_df['label_actual']==x2_test_df['label'])/x2_test_df.shape[0]\n","    print('Test: ',i)\n","\n","    # Print Accuracy\n","    print('Testing Accuracy:',test_accuracy)\n","    print('Training Accuracy:',train_accuracy)\n","    print('\\n')\n","    \n","    add_loss['testing_loss'] = -np.mean(np.multiply(y_test,np.log(x2_test)))\n","    add_loss['test_accuracy'] = test_accuracy\n","    loss_weight_dict[count] = add_loss\n","    count = count + 1\n","    "]},{"cell_type":"code","execution_count":null,"id":"c79d303c","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":5}
